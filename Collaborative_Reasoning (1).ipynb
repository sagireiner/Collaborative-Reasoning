{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "collapsed": true,
        "id": "TDgbrPBIRhdx"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "GdRNNkd9Rhdy"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "def load_model_qwen(model_name):\n",
        "  \"\"\"\n",
        "    Load Qwen‑2.5 (or any chat‑style causal‑LM) and its tokenizer.\n",
        "  \"\"\"\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_name,\n",
        "      torch_dtype=\"auto\",\n",
        "      device_map=\"auto\"\n",
        "  )\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  return model, tokenizer\n",
        "\n",
        "def load_model_llama(model_name):\n",
        "  \"\"\"\n",
        "  Build a text‑generation pipeline for LLaMA.\n",
        "\n",
        "  \"\"\"\n",
        "  pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "  )\n",
        "  return pipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "73Nb8RjdRhdz"
      },
      "outputs": [],
      "source": [
        "instruction_format_letters = \"\"\"\n",
        "**Instruction**: You will be given a question and a number of choices for the correct answer. Your task is:\n",
        "1. Choose the best answer from the choices.\n",
        "2. Provide a short explanation for why each choice is correct or incorrect.\n",
        "3. Output your final answer strictly in this JSON format:\n",
        "{\n",
        "  “answer”: “<Enter the label of the correct answer (A, B, C, or D)>”,\n",
        "  “explanations”: {\n",
        "  “A”: “”,\n",
        "  “B”: “”,\n",
        "  “C”: “”,\n",
        "  “D”: “”\n",
        "  }\n",
        "}\n",
        "**Important**:\n",
        "- Begin the output with `{` and end with `}`.\n",
        "- Do not include any additional characters or text outside of the JSON.\n",
        "\"\"\"\n",
        "question_format = \"The question is: {question}\\nThe choices (possible answers) are:\\n{choices}\"\n",
        "\n",
        "instruction_format_numbers = \"\"\"\n",
        "**Instruction**: You will be given a question and a number of choices for the correct answer. Your task is:\n",
        "1. Choose the best answer from the choices.\n",
        "2. Provide a short explanation for why each choice is correct or incorrect.\n",
        "3. Output your final answer strictly in this JSON format:\n",
        "{\n",
        "  \"answer\": \"<Enter the label of the correct answer (1, 2, 3, or 4)>\",\n",
        "  \"explanations\": {\n",
        "    \"1\": \"\",\n",
        "    \"2\": \"\",\n",
        "    \"3\": \"\",\n",
        "    \"4\": \"\"\n",
        "  }\n",
        "}\n",
        "**Important**:\n",
        "- Begin the output with `{` and end with `}`.\n",
        "- Do not include any additional characters or text outside of the JSON.\n",
        "\"\"\"\n",
        "\n",
        "full_ctx_instruction = \"\"\"\n",
        "You will be given a multiple‑choice question and, below it, the previous\n",
        "answers and explanations of two models. Your task:\n",
        "\n",
        "1. Choose the single best answer.\n",
        "2. Output strictly in this JSON format **without any extra text**:\n",
        "{\n",
        "  \"answer\": \"<Enter the label for the answer you think is correct>\"\n",
        "}\n",
        "\n",
        "Important:\n",
        "- The in the in the json value must be be wrapped in double quotes this format: {\"answer\": \"<label>\"}\n",
        "- Begin with “{” and end with “}”.\n",
        "- Do NOT include explanations or any other keys.\n",
        "\"\"\"\n",
        "\n",
        "two_choices_instruction_format = \"\"\"\n",
        "You will be given a question and two choices for the correct answer. For each choice, you will be given an argument for and an argument against the choice. Based on the arguments given, select the most accurate answer. Organize your answer in the following format:\n",
        "{\n",
        "  \"answer\": \"<Enter the label for the answer you think is correct>\"\n",
        "}\n",
        "Important:\n",
        "- Do not add any additional characters or output other than the dictionary described - your outpout should start with the character for beginning a dictionary - '{' and end with '}'.\\n\n",
        "- The in the in the json value must be be wrapped in double quotes this format: {\"answer\": \"<label>\"}\n",
        "\"\"\"\n",
        "\n",
        "llama_system_prompt =\" You are Llamma, created by Meta. You are a helpful assistant. The token |im_start| indicates the begining of a message, followed by the role of the speaker. The messages where role of the speaker is the user are the user prompts\"\n",
        "qwen_system_prompt = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant. The token |im_start| indicates the begining of a message, followed by the role of the speaker. The messages where role of the speaker is the user are the user prompts\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "def get_model_answer(model_response):\n",
        "  \"\"\"\n",
        "  Parse the model’s JSON‑style answer; return None on failure.\n",
        "  \"\"\"\n",
        "\n",
        "  model_response_clean = model_response.strip()\n",
        "  try:\n",
        "     return ast.literal_eval(model_response_clean)\n",
        "  except:\n",
        "    return None"
      ],
      "metadata": {
        "id": "Ulub1GMsdXGN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "9VxDBfUvRhd0"
      },
      "outputs": [],
      "source": [
        "def add_context(messages, role, content):\n",
        "  \"\"\"\n",
        "    Append a message dict to the conversation.\n",
        "  \"\"\"\n",
        "  messages.append({\"role\": role, \"content\": content})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "8NLolKx7Rhdz"
      },
      "outputs": [],
      "source": [
        "def get_question_prompt(question_idx, dataset, question_label):\n",
        "    \"\"\"Build the Individual Answers with Explanations prompt.\"\"\"\n",
        "    q = dataset[\"test\"][question_idx]\n",
        "    correct = q[\"answerKey\"]\n",
        "\n",
        "    # choose the right instruction block\n",
        "    instr = instruction_format_numbers if str(correct).isdigit() else instruction_format_letters\n",
        "\n",
        "    # build choices string\n",
        "    labels, texts = q[\"choices\"][\"label\"], q[\"choices\"][\"text\"]\n",
        "    choices_str = \"\\n\".join(f\"{l}) {t}\" for l, t in zip(labels, texts))\n",
        "\n",
        "    prompt = instr + question_format.format(question=q[question_label], choices=choices_str)\n",
        "    return prompt, correct"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_full_ctx_prompt(\n",
        "        idx: int,\n",
        "        ds,\n",
        "        question_field: str,\n",
        "        resp1: dict | None,\n",
        "        resp2: dict | None\n",
        ") -> str:\n",
        "    \"\"\"Build the Explanation-Based Answer Selection prompt: question + both prior answers/explanations.\"\"\"\n",
        "    q = ds[\"test\"][idx]\n",
        "    # ----- extract question & choices ---------------------------------------\n",
        "    question_txt = q[question_field]                      # << just this changed\n",
        "    labels, texts = q[\"choices\"][\"label\"], q[\"choices\"][\"text\"]\n",
        "    choices_str = \"\\n\".join(f\"{l}) {t}\" for l, t in zip(labels, texts))\n",
        "\n",
        "    # ----- build prompt ------------------------------------------------------\n",
        "    prompt = (\n",
        "        full_ctx_instruction +\n",
        "        f\"\\nThe question is: {question_txt}\\n\"\n",
        "        f\"The choices (possible answers) are:\\n{choices_str}\\n\\n\"\n",
        "        \"Here are the previous answers:\\n\\n\"\n",
        "    )\n",
        "\n",
        "    def fmt(resp, tag):\n",
        "        if resp is None:\n",
        "            return f\"{tag} model produced an invalid answer.\\n\"\n",
        "        out = [f\"The {tag} model answered: {resp['answer']}\", \"His explanations were:\"]\n",
        "        out += [f\"{k}: {v}\" for k, v in resp[\"explanations\"].items()]\n",
        "        return \"\\n\".join(out)\n",
        "\n",
        "    prompt += fmt(resp1, \"first\") + \"\\n\\n\" + fmt(resp2, \"second\") + \"\\n\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "ebJahUArdRxF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_two_choices_prompt(idx: int,\n",
        "                           dataset,\n",
        "                           question_label,\n",
        "                           resp1: dict,\n",
        "                           resp2: dict):\n",
        "    \"\"\"\n",
        "    Build the Disagreement Resolution prompt with two candidate answers.\n",
        "    \"\"\"\n",
        "    q = dataset[\"test\"][idx]\n",
        "    gold = q[\"answerKey\"]\n",
        "\n",
        "    # no debate if same answer / bad JSON\n",
        "    if resp1 is None or resp2 is None or resp1[\"answer\"] == resp2[\"answer\"]:\n",
        "        return \"\", gold\n",
        "\n",
        "    a1, a2 = resp1[\"answer\"], resp2[\"answer\"]\n",
        "    label2txt = {l: t for l, t in zip(q[\"choices\"][\"label\"], q[\"choices\"][\"text\"])}\n",
        "\n",
        "    choices_block = f\"{a1}) {label2txt[a1]}\\n{a2}) {label2txt[a2]}\"\n",
        "\n",
        "    prompt = two_choices_instruction_format + question_format.format(\n",
        "        question=q[question_label], choices=choices_block\n",
        "    )\n",
        "\n",
        "    #  ── arguments (no model names) ──\n",
        "    prompt += f\"\\nThe argument for answer '{a1}' is:\\n{resp1['explanations'][a1]}\"\n",
        "    prompt += f\"\\nThe argument against answer '{a1}' is:\\n{resp2['explanations'][a1]}\"\n",
        "    prompt += f\"\\nThe argument for answer '{a2}' is:\\n{resp2['explanations'][a2]}\"\n",
        "    prompt += f\"\\nThe argument against answer '{a2}' is:\\n{resp1['explanations'][a2]}\"\n",
        "\n",
        "    return prompt, gold"
      ],
      "metadata": {
        "id": "TEhhDXg2U5oJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "EU6B3UYNRhdz"
      },
      "outputs": [],
      "source": [
        "def generate_response_qwen(model, tokenizer, messages):\n",
        "  \"\"\"\n",
        "  Generate a chat response from Qwen.\n",
        "  \"\"\"\n",
        "\n",
        "  text = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize=False,\n",
        "      add_generation_prompt=True\n",
        "  )\n",
        "  model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "  generated_ids = model.generate(\n",
        "      **model_inputs,\n",
        "       temperature = 0.7,\n",
        "       do_sample = True,\n",
        "       max_new_tokens = 512\n",
        "  )\n",
        "  generated_ids = [\n",
        "      output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "  ]\n",
        "  response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "  add_context(messages, \"assistant\", response)\n",
        "  return response\n",
        "\n",
        "def generate_response_llama(pipe, messages):\n",
        "  \"\"\"\n",
        "  Generate a response from the LLaMA text‑generation pipeline.\n",
        "  \"\"\"\n",
        "  outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=512,\n",
        "  )\n",
        "  response = outputs[0][\"generated_text\"][-1]['content']\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def eval_three_stage(qwen_model, qwen_tok,\n",
        "                     llama_pipe,\n",
        "                     sys_qwen, sys_llama,\n",
        "                     ds,                         # dataset to read from\n",
        "                     question_field,\n",
        "                     start_q=0, end_q=500):\n",
        "    \"\"\"\n",
        "    Three step evaluation fro the three methods:\n",
        "        1. each model answers the MCQ;\n",
        "        2. if they disagree, both re‑answer with a two‑choice debate prompt;\n",
        "        3. both re‑answer again after seeing each other’s full step‑1 answer and explanation .\n",
        "    Returns six accuracy lists (one per model × stage).\n",
        "    \"\"\"\n",
        "    s1q, s1l, s2q, s2l, s3q, s3l = [], [], [], [], [], []\n",
        "\n",
        "    for i in range(start_q, end_q):\n",
        "        base_prompt, gold = get_question_prompt(i,ds, question_field)\n",
        "        # ---- step‑1 ----\n",
        "        mq, ml = [], []\n",
        "        add_context(mq, \"system\", sys_qwen);  add_context(mq, \"user\", base_prompt)\n",
        "        add_context(ml, \"user\", base_prompt)\n",
        "        r1q = get_model_answer(generate_response_qwen(qwen_model, qwen_tok, mq))\n",
        "        r1l = get_model_answer(generate_response_llama(llama_pipe, ml))\n",
        "        for arr, r in ((s1q, r1q), (s1l, r1l)):\n",
        "            arr.append(np.nan if r is None else 1.0 if r[\"answer\"] == gold else 0.0)\n",
        "\n",
        "        # ---- step‑2 : 2‑choice debate (only on disagreement) ----\n",
        "        if r1q is None or r1l is None or r1q[\"answer\"] == r1l[\"answer\"]:\n",
        "            s2q.append(s1q[-1]); s2l.append(s1l[-1])\n",
        "        else:\n",
        "            try:\n",
        "                two_prompt, _ = get_two_choices_prompt(i, ds, question_field, r1q, r1l)\n",
        "            except Exception as e:\n",
        "                # fall back to step‑1 scores\n",
        "                s2q.append(s1q[-1]); s2l.append(s1l[-1])\n",
        "            else:\n",
        "                mq2, ml2 = [], []\n",
        "                add_context(mq2, \"system\", sys_qwen); add_context(mq2, \"user\", two_prompt)\n",
        "                add_context(ml2, \"user\", two_prompt)\n",
        "\n",
        "                r2q = get_model_answer(generate_response_qwen(qwen_model, qwen_tok, mq2))\n",
        "                r2l = get_model_answer(generate_response_llama(llama_pipe, ml2))\n",
        "\n",
        "                s2q.append(s1q[-1] if r2q is None else 1.0 if r2q[\"answer\"] == gold else 0.0)\n",
        "                s2l.append(s1l[-1] if r2l is None else 1.0 if r2l[\"answer\"] == gold else 0.0)\n",
        "\n",
        "        # ---- step‑3 : full‑context prompt ----\n",
        "        full_prompt = get_full_ctx_prompt(i,ds, question_field, r1q, r1l)\n",
        "        mq3, ml3 = [], []\n",
        "        add_context(mq3, \"system\", sys_qwen);  add_context(mq3, \"user\", full_prompt)\n",
        "        add_context(ml3, \"user\", full_prompt)\n",
        "        r3q = get_model_answer(generate_response_qwen(qwen_model, qwen_tok, mq3))\n",
        "        r3l = get_model_answer(generate_response_llama(llama_pipe, ml3))\n",
        "        for arr, r in ((s3q, r3q), (s3l, r3l)):\n",
        "            arr.append(np.nan if r is None else 1.0 if r[\"answer\"] == gold else 0.0)\n",
        "\n",
        "    return dict(individual_answers_with_explanations_qwen=s1q, individual_answers_with_explanations_llama=s1l,\n",
        "                disagreement_resolution_qwen=s2q, disagreement_resolution_llama=s2l,\n",
        "                Explanation_Based_Answer_selection_qwen=s3q, Explanation_Based_Answer_selection_llama=s3l)"
      ],
      "metadata": {
        "id": "ME9kaEB_WrRQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "openbook_ds = load_dataset(\"allenai/openbookqa\", \"additional\")\n",
        "ai2_arc_ds = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\")\n",
        "ai2_arc_easy = load_dataset(\"allenai/ai2_arc\", \"ARC-Easy\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xUgWPBtXgsnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "collapsed": true,
        "id": "u4HZid2LRhd0"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login\n",
        "llama_model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "qwen_model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "qwen_model, qwen_tokenizer = load_model_qwen(qwen_model_name)\n",
        "llama_pipe = load_model_llama(llama_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ≡≡ Run evaluation ≡≡\n",
        "scores_openbook = eval_three_stage(\n",
        "    qwen_model,           # Qwen model obj\n",
        "    qwen_tokenizer,       # Qwen tokenizer\n",
        "    llama_pipe,           # LLaMA generation pipeline\n",
        "    qwen_system_prompt,   # system prompt for Qwen\n",
        "    llama_system_prompt,  # system prompt for LLaMA\n",
        "    openbook_ds,          # dataset\n",
        "    \"question_stem\",           # question field name\n",
        "    start_q=0,            # first question index\n",
        "    end_q=500   # run on the whole test split\n",
        ")\n",
        "\n",
        "# quick summary\n",
        "import numpy as np\n",
        "for k, v in scores_openbook.items():\n",
        "    print(f\"{k:12s}: {np.nanmean(v):.3f}\")"
      ],
      "metadata": {
        "id": "E-gqOK0Dfqw8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ≡≡ Run evaluation ≡≡\n",
        "scores_arceasy = eval_three_stage(\n",
        "    qwen_model,           # Qwen model obj\n",
        "    qwen_tokenizer,       # Qwen tokenizer\n",
        "    llama_pipe,           # LLaMA generation pipeline\n",
        "    qwen_system_prompt,   # system prompt for Qwen\n",
        "    llama_system_prompt,  # system prompt for LLaMA\n",
        "    ai2_arc_easy,          # dataset\n",
        "    \"question\",           # question field name\n",
        "    start_q=0,            # first question index\n",
        "    end_q=500   # run on the whole test split\n",
        ")\n",
        "\n",
        "# quick summary\n",
        "import numpy as np\n",
        "for k, v in scores_arceasy.items():\n",
        "    print(f\"{k:12s}: {np.nanmean(v):.3f}\")"
      ],
      "metadata": {
        "id": "QAf9mgnV4DSO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ≡≡ Run evaluation ≡≡\n",
        "scores_arcchallenge = eval_three_stage(\n",
        "    qwen_model,           # Qwen model obj\n",
        "    qwen_tokenizer,       # Qwen tokenizer\n",
        "    llama_pipe,           # LLaMA generation pipeline\n",
        "    qwen_system_prompt,   # system prompt for Qwen\n",
        "    llama_system_prompt,  # system prompt for LLaMA\n",
        "    ai2_arc_ds,          # dataset\n",
        "    \"question\",           # question field name\n",
        "    start_q=0,            # first question index\n",
        "    end_q=500   # run on the whole test split\n",
        ")\n",
        "\n",
        "# quick summary\n",
        "import numpy as np\n",
        "for k, v in scores_arcchallenge.items():\n",
        "    print(f\"{k:12s}: {np.nanmean(v):.3f}\")"
      ],
      "metadata": {
        "id": "juuwx4ZZlrMO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}